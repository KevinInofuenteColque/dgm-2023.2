{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cpFJHF0CGYS",
    "outputId": "ab98b3ec-c552-47f7-cd90-8dd202ecd899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.4/184.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.3/239.3 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.5/227.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.0/365.0 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pyriemann (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "! python3.7 -m pip install -q braindecode moabb mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2Zq4vte7Kmb"
   },
   "source": [
    "## Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NDkpht8DtOWh",
    "outputId": "73bf9da7-49bc-47f8-823c-72d9af891178"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/Mestrado/venv_BCI/venv_BCI/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "from skorch.helper import predefined_split\n",
    "from skorch.callbacks import LRScheduler\n",
    "from braindecode import EEGClassifier\n",
    "from braindecode.datasets import MOABBDataset\n",
    "\n",
    "subject_id = 3\n",
    "dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=[subject_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "ek_mLVTAGrpE",
    "outputId": "894fa45d-d6f1-484e-ba9a-bc845cab1ae3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/Mestrado/venv_BCI/venv_BCI/lib/python3.7/site-packages/braindecode/preprocessing/preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "FIR filter parameters\n",
      "Filtering raw data in 1 contiguous segment\n",
      "---------------------\n",
      "- Upper passband edge: 38.00 Hz\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "Filtering raw data in 1 contiguous segment\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "FIR filter parameters\n",
      "\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n"
     ]
    }
   ],
   "source": [
    "from braindecode.preprocessing import preprocess, Preprocessor\n",
    "import numpy as np\n",
    "from numpy import multiply\n",
    "\n",
    "low_cut_hz = 4.\n",
    "high_cut_hz = 38.\n",
    "newfreq = 128\n",
    "factor = 1e6\n",
    "\n",
    "preprocessors = [\n",
    "    Preprocessor('pick_types', eeg=True, meg=False, stim=False),\n",
    "    Preprocessor(lambda data: multiply(data, factor)),\n",
    "    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "    Preprocessor(\"set_eeg_reference\", ref_channels=\"average\", ch_type=\"eeg\")\n",
    "]\n",
    "\n",
    "preprocess(dataset, preprocessors, n_jobs=-1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "W_IF-6T7x5dk",
    "outputId": "c8d87df8-e31a-447c-fff6-e0c382f70733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
     ]
    }
   ],
   "source": [
    "from braindecode.preprocessing import create_windows_from_events\n",
    "\n",
    "sfreq = dataset.datasets[0].raw.info['sfreq']\n",
    "\n",
    "trial_start_offset_seconds = -0.5\n",
    "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "trial_stop_offset_seconds = -2\n",
    "trial_stop_offset_samples = int(trial_stop_offset_seconds * sfreq)\n",
    "\n",
    "windows_dataset = create_windows_from_events(\n",
    "    dataset,\n",
    "    trial_start_offset_samples=trial_start_offset_samples,\n",
    "    trial_stop_offset_samples=trial_stop_offset_samples,\n",
    "    preload=True,\n",
    ")\n",
    "\n",
    "trial_start_offset_seconds = -0.5\n",
    "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "windows_dataset = create_windows_from_events(\n",
    "    dataset,\n",
    "    trial_start_offset_samples=trial_start_offset_samples,\n",
    "    trial_stop_offset_samples=0,\n",
    "    preload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MJxP7MGGCGYr",
    "outputId": "6ecc9714-a76f-4506-daba-d1f017a2af58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([576, 1, 22, 1125])\n",
      "torch.Size([576])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/Mestrado/venv_BCI/venv_BCI/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "X_ = []\n",
    "y_ = []\n",
    "for X, y, (i_window_in_trial, i_start_sample, i_stop_sample) in windows_dataset:\n",
    "    X_.append(X)\n",
    "    y_.append(y)\n",
    "X_ = torch.as_tensor(X_).float()\n",
    "y_ = torch.as_tensor(y_)\n",
    "\n",
    "real_set = TensorDataset(X_, y_)\n",
    "\n",
    "X_ = torch.from_numpy(np.expand_dims(X_, axis=1))\n",
    "\n",
    "gen_set = TensorDataset(X_, y_)\n",
    "gen_loader = DataLoader(dataset=gen_set, batch_size=576, shuffle=True)\n",
    "\n",
    "print(X_.shape)\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do modelo do VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets,transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(22*1125, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 22*1125)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 22*1125))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 22*1125), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 576\n",
    "    for batch_idx, (data, _) in enumerate(gen_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(gen_loader.dataset),\n",
    "                100. * batch_idx / len(gen_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(gen_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 576\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(gen_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n], recon_batch.view(576, 1, 22, 1125)[:n]])\n",
    "\n",
    "    test_loss /= len(gen_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/576 (0%)]\tLoss: 17545.581597\n",
      "====> Epoch: 1 Average loss: 17546.5816\n",
      "====> Test set loss: 1867217924552.1111\n",
      "Train Epoch: 2 [0/576 (0%)]\tLoss: 1867217924551.111084\n",
      "====> Epoch: 2 Average loss: 1867217924552.1111\n",
      "====> Test set loss: 930441671824726.3750\n",
      "Train Epoch: 3 [0/576 (0%)]\tLoss: 930441612172401.750000\n",
      "====> Epoch: 3 Average loss: 930441612172402.7500\n",
      "====> Test set loss: 10052464399505367040.0000\n",
      "Train Epoch: 4 [0/576 (0%)]\tLoss: 10052464399505367040.000000\n",
      "====> Epoch: 4 Average loss: 10052464399505367040.0000\n",
      "====> Test set loss: 23486066585501696.0000\n",
      "Train Epoch: 5 [0/576 (0%)]\tLoss: 23486064676627344.000000\n",
      "====> Epoch: 5 Average loss: 23486064676627344.0000\n",
      "====> Test set loss: 1024033679.2222\n",
      "Train Epoch: 6 [0/576 (0%)]\tLoss: 1024034816.000000\n",
      "====> Epoch: 6 Average loss: 1024034817.0000\n",
      "====> Test set loss: 1205589537224.1111\n",
      "Train Epoch: 7 [0/576 (0%)]\tLoss: 1205589304206.222168\n",
      "====> Epoch: 7 Average loss: 1205589304207.2222\n",
      "====> Test set loss: 15593012389332766.0000\n",
      "Train Epoch: 8 [0/576 (0%)]\tLoss: 15593013343769942.000000\n",
      "====> Epoch: 8 Average loss: 15593013343769944.0000\n",
      "====> Test set loss: 53902868481.0000\n",
      "Train Epoch: 9 [0/576 (0%)]\tLoss: 53902864839.111115\n",
      "====> Epoch: 9 Average loss: 53902864840.1111\n",
      "====> Test set loss: 592916367133.4445\n",
      "Train Epoch: 10 [0/576 (0%)]\tLoss: 592916367132.444458\n",
      "====> Epoch: 10 Average loss: 592916367133.4445\n",
      "====> Test set loss: 841926544953.8889\n",
      "Train Epoch: 11 [0/576 (0%)]\tLoss: 841926486698.666626\n",
      "====> Epoch: 11 Average loss: 841926486699.6666\n",
      "====> Test set loss: 48774498077.4444\n",
      "Train Epoch: 12 [0/576 (0%)]\tLoss: 48774498076.444443\n",
      "====> Epoch: 12 Average loss: 48774498077.4444\n",
      "====> Test set loss: 1422519638.3333\n",
      "Train Epoch: 13 [0/576 (0%)]\tLoss: 1422519978.666667\n",
      "====> Epoch: 13 Average loss: 1422519979.6667\n",
      "====> Test set loss: 28394184.1111\n",
      "Train Epoch: 14 [0/576 (0%)]\tLoss: 28391248.000000\n",
      "====> Epoch: 14 Average loss: 28391249.0000\n",
      "====> Test set loss: 355141.1111\n",
      "Train Epoch: 15 [0/576 (0%)]\tLoss: 351927.722222\n",
      "====> Epoch: 15 Average loss: 351928.7222\n",
      "====> Test set loss: -218415.3194\n",
      "Train Epoch: 16 [0/576 (0%)]\tLoss: -216369.958333\n",
      "====> Epoch: 16 Average loss: -216368.9583\n",
      "====> Test set loss: -261818.1667\n",
      "Train Epoch: 17 [0/576 (0%)]\tLoss: -263308.861111\n",
      "====> Epoch: 17 Average loss: -263307.8611\n",
      "====> Test set loss: -294228.3611\n",
      "Train Epoch: 18 [0/576 (0%)]\tLoss: -293558.027778\n",
      "====> Epoch: 18 Average loss: -293557.0278\n",
      "====> Test set loss: -324914.3611\n",
      "Train Epoch: 19 [0/576 (0%)]\tLoss: -325007.305556\n",
      "====> Epoch: 19 Average loss: -325006.3056\n",
      "====> Test set loss: -353362.5000\n",
      "Train Epoch: 20 [0/576 (0%)]\tLoss: -353487.472222\n",
      "====> Epoch: 20 Average loss: -353486.4722\n",
      "====> Test set loss: -381011.5833\n",
      "Train Epoch: 21 [0/576 (0%)]\tLoss: -382950.277778\n",
      "====> Epoch: 21 Average loss: -382949.2778\n",
      "====> Test set loss: -407473.6389\n",
      "Train Epoch: 22 [0/576 (0%)]\tLoss: -407764.277778\n",
      "====> Epoch: 22 Average loss: -407763.2778\n",
      "====> Test set loss: -437990.3333\n",
      "Train Epoch: 23 [0/576 (0%)]\tLoss: -435823.694444\n",
      "====> Epoch: 23 Average loss: -435822.6944\n",
      "====> Test set loss: -459364.6111\n",
      "Train Epoch: 24 [0/576 (0%)]\tLoss: -460791.277778\n",
      "====> Epoch: 24 Average loss: -460790.2778\n",
      "====> Test set loss: -487159.0000\n",
      "Train Epoch: 25 [0/576 (0%)]\tLoss: -487531.611111\n",
      "====> Epoch: 25 Average loss: -487530.6111\n",
      "====> Test set loss: -507797.9444\n",
      "Train Epoch: 26 [0/576 (0%)]\tLoss: -508502.611111\n",
      "====> Epoch: 26 Average loss: -508501.6111\n",
      "====> Test set loss: -528548.3889\n",
      "Train Epoch: 27 [0/576 (0%)]\tLoss: -525432.833333\n",
      "====> Epoch: 27 Average loss: -525431.8333\n",
      "====> Test set loss: -548011.5556\n",
      "Train Epoch: 28 [0/576 (0%)]\tLoss: -548720.666667\n",
      "====> Epoch: 28 Average loss: -548719.6667\n",
      "====> Test set loss: -566408.1111\n",
      "Train Epoch: 29 [0/576 (0%)]\tLoss: -567583.888889\n",
      "====> Epoch: 29 Average loss: -567582.8889\n",
      "====> Test set loss: -583489.7778\n",
      "Train Epoch: 30 [0/576 (0%)]\tLoss: -582763.388889\n",
      "====> Epoch: 30 Average loss: -582762.3889\n",
      "====> Test set loss: -600673.0556\n",
      "Train Epoch: 31 [0/576 (0%)]\tLoss: -598728.277778\n",
      "====> Epoch: 31 Average loss: -598727.2778\n",
      "====> Test set loss: -613610.6667\n",
      "Train Epoch: 32 [0/576 (0%)]\tLoss: -613513.444444\n",
      "====> Epoch: 32 Average loss: -613512.4444\n",
      "====> Test set loss: -627267.8333\n",
      "Train Epoch: 33 [0/576 (0%)]\tLoss: -628334.444444\n",
      "====> Epoch: 33 Average loss: -628333.4444\n",
      "====> Test set loss: -642464.8333\n",
      "Train Epoch: 34 [0/576 (0%)]\tLoss: -642705.888889\n",
      "====> Epoch: 34 Average loss: -642704.8889\n",
      "====> Test set loss: -657550.1667\n",
      "Train Epoch: 35 [0/576 (0%)]\tLoss: -655206.555556\n",
      "====> Epoch: 35 Average loss: -655205.5556\n",
      "====> Test set loss: -668673.3333\n",
      "Train Epoch: 36 [0/576 (0%)]\tLoss: -669687.555556\n",
      "====> Epoch: 36 Average loss: -669686.5556\n",
      "====> Test set loss: -680598.8333\n",
      "Train Epoch: 37 [0/576 (0%)]\tLoss: -681132.166667\n",
      "====> Epoch: 37 Average loss: -681131.1667\n",
      "====> Test set loss: -694014.7778\n",
      "Train Epoch: 38 [0/576 (0%)]\tLoss: -685839.666667\n",
      "====> Epoch: 38 Average loss: -685838.6667\n",
      "====> Test set loss: -706431.6111\n",
      "Train Epoch: 39 [0/576 (0%)]\tLoss: -701288.555556\n",
      "====> Epoch: 39 Average loss: -701287.5556\n",
      "====> Test set loss: -711476.8333\n",
      "Train Epoch: 40 [0/576 (0%)]\tLoss: -716213.944444\n",
      "====> Epoch: 40 Average loss: -716212.9444\n",
      "====> Test set loss: -725458.1667\n",
      "Train Epoch: 41 [0/576 (0%)]\tLoss: -726319.388889\n",
      "====> Epoch: 41 Average loss: -726318.3889\n",
      "====> Test set loss: -738725.5556\n",
      "Train Epoch: 42 [0/576 (0%)]\tLoss: -732817.166667\n",
      "====> Epoch: 42 Average loss: -732816.1667\n",
      "====> Test set loss: -749331.0000\n",
      "Train Epoch: 43 [0/576 (0%)]\tLoss: -743368.833333\n",
      "====> Epoch: 43 Average loss: -743367.8333\n",
      "====> Test set loss: -758081.8889\n",
      "Train Epoch: 44 [0/576 (0%)]\tLoss: -762996.277778\n",
      "====> Epoch: 44 Average loss: -762995.2778\n",
      "====> Test set loss: -774871.8889\n",
      "Train Epoch: 45 [0/576 (0%)]\tLoss: -775827.833333\n",
      "====> Epoch: 45 Average loss: -775826.8333\n",
      "====> Test set loss: -784126.3333\n",
      "Train Epoch: 46 [0/576 (0%)]\tLoss: -782695.000000\n",
      "====> Epoch: 46 Average loss: -782694.0000\n",
      "====> Test set loss: -800442.8889\n",
      "Train Epoch: 47 [0/576 (0%)]\tLoss: -800227.277778\n",
      "====> Epoch: 47 Average loss: -800226.2778\n",
      "====> Test set loss: -815537.5556\n",
      "Train Epoch: 48 [0/576 (0%)]\tLoss: -812379.111111\n",
      "====> Epoch: 48 Average loss: -812378.1111\n",
      "====> Test set loss: -830520.6111\n",
      "Train Epoch: 49 [0/576 (0%)]\tLoss: -829830.277778\n",
      "====> Epoch: 49 Average loss: -829829.2778\n",
      "====> Test set loss: -843837.8333\n",
      "Train Epoch: 50 [0/576 (0%)]\tLoss: -843899.888889\n",
      "====> Epoch: 50 Average loss: -843898.8889\n",
      "====> Test set loss: -858558.1111\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 51):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(576, 20).to(device)\n",
    "        sample = model.decode(sample).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento dos dados sintéticos para classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([576, 24750])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([576, 22, 1125])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size=(22, 1125)\n",
    "fake = sample.detach().cpu().view(-1, *size)\n",
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_set = TensorDataset(fake, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "fake_fulltrainset, fake_evalset = random_split(fake_set, [288, 288])\n",
    "fake_trainset, fake_testset = random_split(fake_fulltrainset, [192, 96])\n",
    "real_fulltrainset, real_evalset = random_split(real_set, [288, 288])\n",
    "real_trainset, real_testset = random_split(real_fulltrainset, [192, 96])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do modelo do classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import EEGNetv4\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 20200220\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes = 4\n",
    "n_chans = 22\n",
    "input_window_samples = 1125\n",
    "F1, D = 4, 2\n",
    "kernel_length = 64\n",
    "\n",
    "model = EEGNetv4(\n",
    "    n_chans,\n",
    "    n_classes,\n",
    "    input_window_samples=input_window_samples,\n",
    "    final_conv_length='auto',\n",
    "    F1=8,\n",
    "    D=2,\n",
    "    F2=F1*D,\n",
    "    kernel_length=kernel_length,\n",
    "    drop_prob=0.5\n",
    ")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do classificador com dados reais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4427\u001b[0m        \u001b[32m1.4184\u001b[0m            \u001b[35m0.4062\u001b[0m        \u001b[31m1.3451\u001b[0m  0.0100  1.2378\n",
      "      2            0.3333        \u001b[32m1.2695\u001b[0m            0.2188        1.5929  0.0100  0.0844\n",
      "      3            0.3542        \u001b[32m1.0458\u001b[0m            0.2292        2.4482  0.0100  0.0833\n",
      "      4            0.3490        \u001b[32m0.9074\u001b[0m            0.2292        3.5232  0.0099  0.0778\n",
      "      5            0.3490        \u001b[32m0.7536\u001b[0m            0.2292        4.4249  0.0098  0.0774\n",
      "      6            0.3542        \u001b[32m0.6959\u001b[0m            0.2292        4.9226  0.0097  0.0762\n",
      "      7            0.3854        \u001b[32m0.5595\u001b[0m            0.2396        4.4832  0.0096  0.0767\n",
      "      8            0.3958        0.5742            0.2500        4.9524  0.0095  0.0773\n",
      "      9            0.3958        \u001b[32m0.5449\u001b[0m            0.2604        4.8418  0.0094  0.0750\n",
      "     10            \u001b[36m0.4479\u001b[0m        \u001b[32m0.4446\u001b[0m            0.2812        4.0662  0.0092  0.0743\n",
      "     11            \u001b[36m0.6927\u001b[0m        0.4504            \u001b[35m0.4167\u001b[0m        2.7005  0.0090  0.0731\n",
      "     12            \u001b[36m0.7760\u001b[0m        \u001b[32m0.4230\u001b[0m            \u001b[35m0.4792\u001b[0m        2.2817  0.0088  0.0735\n",
      "     13            \u001b[36m0.8802\u001b[0m        \u001b[32m0.4216\u001b[0m            \u001b[35m0.5312\u001b[0m        1.7163  0.0086  0.0743\n",
      "     14            \u001b[36m0.9115\u001b[0m        \u001b[32m0.3223\u001b[0m            \u001b[35m0.5521\u001b[0m        1.6724  0.0084  0.0738\n",
      "     15            0.8490        0.3451            0.5104        2.2117  0.0081  0.0733\n",
      "     16            0.8177        \u001b[32m0.3201\u001b[0m            0.5000        2.3890  0.0079  0.0735\n",
      "     17            \u001b[36m0.9375\u001b[0m        0.3252            \u001b[35m0.5625\u001b[0m        1.6514  0.0076  0.0741\n",
      "     18            \u001b[36m0.9688\u001b[0m        \u001b[32m0.2640\u001b[0m            \u001b[35m0.5938\u001b[0m        1.4186  0.0073  0.0732\n",
      "     19            0.9219        0.3205            0.5417        1.7834  0.0070  0.0751\n",
      "     20            \u001b[36m0.9844\u001b[0m        0.2886            0.5833        1.5222  0.0067  0.0741\n",
      "     21            0.9635        \u001b[32m0.2587\u001b[0m            \u001b[35m0.6146\u001b[0m        \u001b[31m1.2958\u001b[0m  0.0064  0.0739\n",
      "     22            0.9531        \u001b[32m0.2165\u001b[0m            \u001b[35m0.6458\u001b[0m        \u001b[31m1.1120\u001b[0m  0.0061  0.0750\n",
      "     23            0.9844        \u001b[32m0.2013\u001b[0m            0.6458        \u001b[31m1.0230\u001b[0m  0.0058  0.0745\n",
      "     24            \u001b[36m0.9948\u001b[0m        \u001b[32m0.1885\u001b[0m            0.6354        1.1571  0.0055  0.0734\n",
      "     25            0.9948        0.1967            0.6458        1.1074  0.0052  0.0737\n",
      "     26            0.9896        \u001b[32m0.1648\u001b[0m            \u001b[35m0.6562\u001b[0m        \u001b[31m1.0185\u001b[0m  0.0048  0.0747\n",
      "     27            0.9948        0.2551            0.6562        1.0979  0.0045  0.0728\n",
      "     28            0.9948        0.2305            0.6562        1.1147  0.0042  0.0737\n",
      "     29            0.9948        \u001b[32m0.1591\u001b[0m            0.6250        1.1688  0.0039  0.0737\n",
      "     30            \u001b[36m1.0000\u001b[0m        0.1937            0.6458        1.0774  0.0036  0.0745\n",
      "     31            0.9948        0.1710            \u001b[35m0.6771\u001b[0m        1.0346  0.0033  0.0738\n",
      "     32            0.9948        \u001b[32m0.1144\u001b[0m            \u001b[35m0.7083\u001b[0m        1.0288  0.0030  0.0742\n",
      "     33            1.0000        0.2199            0.6979        \u001b[31m0.9351\u001b[0m  0.0027  0.0751\n",
      "     34            0.9948        0.1882            0.6979        \u001b[31m0.8707\u001b[0m  0.0024  0.0737\n",
      "     35            1.0000        0.1956            0.6979        0.8799  0.0021  0.0744\n",
      "     36            1.0000        0.1809            0.6562        0.9175  0.0019  0.0740\n",
      "     37            0.9948        0.2011            0.6562        0.9544  0.0016  0.0768\n",
      "     38            0.9948        0.1631            0.6562        0.9296  0.0014  0.0765\n",
      "     39            1.0000        0.1537            0.6562        0.8719  0.0012  0.0762\n",
      "     40            1.0000        0.1989            \u001b[35m0.7188\u001b[0m        \u001b[31m0.8424\u001b[0m  0.0010  0.0768\n",
      "     41            1.0000        0.1502            0.6979        \u001b[31m0.8384\u001b[0m  0.0008  0.0747\n",
      "     42            1.0000        0.1650            0.7083        0.8452  0.0006  0.0762\n",
      "     43            1.0000        0.1512            0.7083        0.8506  0.0005  0.0761\n",
      "     44            1.0000        0.1662            0.7083        0.8452  0.0004  0.0758\n",
      "     45            0.9948        0.1638            0.7083        \u001b[31m0.8355\u001b[0m  0.0003  0.0731\n",
      "     46            0.9948        0.1434            0.7083        \u001b[31m0.8292\u001b[0m  0.0002  0.0775\n",
      "     47            0.9948        0.1790            0.7083        \u001b[31m0.8224\u001b[0m  0.0001  0.0765\n",
      "     48            0.9948        0.1726            0.7083        \u001b[31m0.8188\u001b[0m  0.0000  0.0741\n",
      "     49            0.9948        0.1179            0.7083        \u001b[31m0.8155\u001b[0m  0.0000  0.0742\n",
      "     50            0.9948        0.1278            0.7083        \u001b[31m0.8132\u001b[0m  0.0000  0.0736\n"
     ]
    }
   ],
   "source": [
    "from braindecode import EEGClassifier\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 50\n",
    "\n",
    "real_clf = EEGClassifier(\n",
    "    model,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    train_split=predefined_split(real_testset),\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "    ],\n",
    "    device=device,\n",
    ")\n",
    "real_clf.fit(real_trainset, y=None, epochs=n_epochs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 79.51%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Accuracy: {np.mean(real_clf.predict(real_evalset) == [y for X,y in real_evalset])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 23.61%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Accuracy: {np.mean(real_clf.predict(fake_evalset) == [y for X,y in fake_evalset])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do classificador com dados sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2917\u001b[0m        \u001b[32m3.5039\u001b[0m            \u001b[35m0.2083\u001b[0m        \u001b[31m6.8457\u001b[0m  0.0100  0.0871\n",
      "      2            0.2917        \u001b[32m2.0838\u001b[0m            0.2083        \u001b[31m1.7949\u001b[0m  0.0100  0.0853\n",
      "      3            0.2917        \u001b[32m1.6909\u001b[0m            0.2083        \u001b[31m1.6415\u001b[0m  0.0100  0.0818\n",
      "      4            0.2708        \u001b[32m1.6136\u001b[0m            \u001b[35m0.2917\u001b[0m        \u001b[31m1.5484\u001b[0m  0.0099  0.0745\n",
      "      5            0.2448        \u001b[32m1.5297\u001b[0m            0.2604        \u001b[31m1.4694\u001b[0m  0.0098  0.0736\n",
      "      6            0.2500        \u001b[32m1.4346\u001b[0m            0.2708        \u001b[31m1.4452\u001b[0m  0.0097  0.0745\n",
      "      7            0.2448        1.4464            0.2604        1.4683  0.0096  0.0735\n",
      "      8            0.2448        \u001b[32m1.3993\u001b[0m            0.2604        \u001b[31m1.4440\u001b[0m  0.0095  0.0733\n",
      "      9            0.2448        \u001b[32m1.3675\u001b[0m            0.2604        \u001b[31m1.4308\u001b[0m  0.0094  0.0745\n",
      "     10            0.2448        \u001b[32m1.3503\u001b[0m            0.2604        1.4478  0.0092  0.0743\n",
      "     11            0.2448        1.3627            0.2604        \u001b[31m1.4285\u001b[0m  0.0090  0.0742\n",
      "     12            0.2448        \u001b[32m1.2939\u001b[0m            0.2604        \u001b[31m1.4202\u001b[0m  0.0088  0.0761\n",
      "     13            0.2448        1.3151            0.2604        \u001b[31m1.4105\u001b[0m  0.0086  0.0741\n",
      "     14            0.2760        1.3103            0.2292        1.4319  0.0084  0.0742\n",
      "     15            0.2865        \u001b[32m1.2838\u001b[0m            0.2500        1.4454  0.0081  0.0744\n",
      "     16            0.2865        1.3054            0.2500        1.4489  0.0079  0.0738\n",
      "     17            0.2812        \u001b[32m1.2389\u001b[0m            0.2604        1.4392  0.0076  0.0756\n",
      "     18            0.2760        1.2926            0.2396        1.4504  0.0073  0.0748\n",
      "     19            \u001b[36m0.3073\u001b[0m        \u001b[32m1.2208\u001b[0m            0.2292        1.4580  0.0070  0.0759\n",
      "     20            \u001b[36m0.3229\u001b[0m        1.2890            0.2500        1.4668  0.0067  0.0744\n",
      "     21            0.3229        1.2863            0.2292        1.4603  0.0064  0.0752\n",
      "     22            \u001b[36m0.3438\u001b[0m        1.2789            0.2188        1.4544  0.0061  0.0744\n",
      "     23            0.3125        1.2413            0.2396        1.4773  0.0058  0.0743\n",
      "     24            0.3385        1.2243            0.2292        1.4686  0.0055  0.0748\n",
      "     25            0.3438        \u001b[32m1.2048\u001b[0m            0.2292        1.4559  0.0052  0.0746\n",
      "     26            \u001b[36m0.3646\u001b[0m        1.2320            0.2396        1.4344  0.0048  0.0748\n",
      "     27            0.3646        1.2607            0.2396        1.4443  0.0045  0.0744\n",
      "     28            0.3594        \u001b[32m1.2002\u001b[0m            0.2083        1.4479  0.0042  0.0752\n",
      "     29            0.3542        1.2902            0.1979        1.4487  0.0039  0.0744\n",
      "     30            \u001b[36m0.3698\u001b[0m        1.2576            0.2292        1.4381  0.0036  0.0748\n",
      "     31            \u001b[36m0.3802\u001b[0m        1.2300            0.2292        1.4307  0.0033  0.0746\n",
      "     32            0.3646        1.2075            0.2292        1.4285  0.0030  0.0753\n",
      "     33            0.3698        1.2255            0.2292        1.4324  0.0027  0.0754\n",
      "     34            0.3802        \u001b[32m1.1666\u001b[0m            0.2188        1.4283  0.0024  0.0761\n",
      "     35            \u001b[36m0.3958\u001b[0m        1.2183            0.2292        1.4274  0.0021  0.0750\n",
      "     36            \u001b[36m0.4115\u001b[0m        1.2258            0.2292        1.4282  0.0019  0.0753\n",
      "     37            0.4115        1.2114            0.2396        1.4256  0.0016  0.0773\n",
      "     38            \u001b[36m0.4219\u001b[0m        1.2320            0.2396        1.4275  0.0014  0.0765\n",
      "     39            0.4167        1.2467            0.2500        1.4294  0.0012  0.0769\n",
      "     40            0.4115        1.2144            0.2604        1.4295  0.0010  0.0775\n",
      "     41            0.4167        1.2029            0.2500        1.4295  0.0008  0.0792\n",
      "     42            0.4219        1.2212            0.2500        1.4298  0.0006  0.0764\n",
      "     43            0.4219        1.2102            0.2500        1.4299  0.0005  0.0752\n",
      "     44            0.4219        1.1706            0.2396        1.4302  0.0004  0.0747\n",
      "     45            \u001b[36m0.4271\u001b[0m        1.2852            0.2396        1.4303  0.0003  0.0747\n",
      "     46            \u001b[36m0.4323\u001b[0m        \u001b[32m1.1574\u001b[0m            0.2396        1.4306  0.0002  0.0743\n",
      "     47            0.4271        1.1789            0.2396        1.4308  0.0001  0.0742\n",
      "     48            0.4271        1.1904            0.2396        1.4309  0.0000  0.0743\n",
      "     49            0.4271        1.1778            0.2396        1.4312  0.0000  0.0743\n",
      "     50            0.4271        1.2448            0.2396        1.4316  0.0000  0.0746\n"
     ]
    }
   ],
   "source": [
    "fake_clf = EEGClassifier(\n",
    "    model,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    train_split=predefined_split(fake_testset),\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "    ],\n",
    "    device=device,\n",
    ")\n",
    "fake_clf.fit(fake_trainset, y=None, epochs=n_epochs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 35.07%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Accuracy: {np.mean(fake_clf.predict(real_evalset) == [y for X,y in real_evalset])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 25.35%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Accuracy: {np.mean(fake_clf.predict(fake_evalset) == [y for X,y in fake_evalset])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distância euclidiana entre os dados reais e sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=(22, 1125)\n",
    "real = X_.view(-1, *size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(113.4993)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.cdist(real, fake, p=2))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "venv_BCI",
   "language": "python",
   "name": "venv_bci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
