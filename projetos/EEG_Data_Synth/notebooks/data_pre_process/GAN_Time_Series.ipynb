{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4959f9e7-c299-4806-bc1e-0af7bfc2c023",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da9e354-8014-49ae-9dee-643e6fd0981a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-10 18:48:00.008513: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-10 18:48:00.008546: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-10 18:48:00.008573: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-10 18:48:00.015250: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow not install, you could not use those pipelines\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0) # Set for our testing purposes, please do not change!\n",
    "\n",
    "from braindecode.datasets import MOABBDataset\n",
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize, preprocess, Preprocessor)\n",
    "from braindecode.preprocessing import \\\n",
    "    create_windows_from_events, create_fixed_length_windows\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42237e-d481-482a-9765-41ab1e15cbde",
   "metadata": {},
   "source": [
    "# Creating function to get EEG Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4134d512-81b7-4067-b8f8-eddcbb13def3",
   "metadata": {},
   "source": [
    "## Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3c27fe-273a-4046-b51a-4965537415ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(\n",
    "    dataset,\n",
    "    low_cut_hz = 4.,   # low cut frequency for filtering\n",
    "    high_cut_hz = 38., # high cut frequency for filtering\n",
    "    newfreq = 100, # Paramater for resampling\n",
    "    factor = 1e6, # Parameter for scaling\n",
    "    ):\n",
    "\n",
    "    preprocessors = [\n",
    "        Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "        Preprocessor(lambda data: np.multiply(data, factor)),  # Convert from V to uV\n",
    "        Preprocessor(\"resample\", sfreq=newfreq), # Resampling\n",
    "        Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
    "        Preprocessor(\"set_eeg_reference\", ref_channels=\"average\", ch_type=\"eeg\") # Common Average Reference\n",
    "    ]\n",
    "    \n",
    "    # Transform the data\n",
    "    # return preprocess(dataset, preprocessors, n_jobs = -1)\n",
    "    return preprocess(dataset, preprocessors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5754a92-a3d8-483e-b757-64be8e64d45a",
   "metadata": {},
   "source": [
    "## Get Windows from Dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee26c93-78cb-4d5e-86d3-ae876e0a1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(\n",
    "        dataset, \n",
    "        trial_start_offset_samples=0,\n",
    "        trial_stop_offset_samples=100,\n",
    "        window_size_samples=400,\n",
    "        window_stride_samples=100,\n",
    "        preload=True,\n",
    "        # mapping = {'left_hand': 0, 'right_hand': 1},\n",
    "        picks = ['C3', 'Cz', 'C4']\n",
    "        ):\n",
    "    \n",
    "    windows_dataset = create_windows_from_events(\n",
    "        dataset,\n",
    "        trial_start_offset_samples = trial_start_offset_samples,\n",
    "        trial_stop_offset_samples  = trial_stop_offset_samples,\n",
    "        window_size_samples        = window_size_samples,\n",
    "        window_stride_samples      = window_stride_samples,\n",
    "        preload                    = True,\n",
    "        # mapping = {'left_hand': 0, 'right_hand': 1},\n",
    "        picks                      = picks\n",
    "        )\n",
    "    \n",
    "    return windows_dataset\n",
    "\n",
    "\n",
    "def get_tensors_from_windows(windows_dataset):\n",
    "    windows_list = []\n",
    "    labels_list = []\n",
    "    n_runs = len(windows_dataset.datasets)\n",
    "    for i in range(n_runs):\n",
    "        windows_list.append(windows_dataset.datasets[i].windows.get_data())\n",
    "        labels_list.append(windows_dataset.datasets[i].y)\n",
    "        \n",
    "    stacked_tensor = np.concatenate(windows_list, axis=0)\n",
    "    stacked_labels = np.concatenate(labels_list, axis=0)\n",
    "    \n",
    "    del windows_list,labels_list\n",
    "    \n",
    "    return stacked_tensor, stacked_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f067c7e2-f452-434d-97d6-8fd6b8850722",
   "metadata": {},
   "source": [
    "# Creating EEG Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9691fcb9-4f26-4aad-8941-d70869e98c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG(Dataset):\n",
    "\n",
    "    def __init__(self, subject_id = 3, dataset_name=\"BNCI2014_001\", transform = None):\n",
    "        \n",
    "        self.raw_dataset     = MOABBDataset(dataset_name = dataset_name, subject_ids=[subject_id])\n",
    "        self.prepro_dataset  = preprocessor(self.raw_dataset)\n",
    "        self.windows_dataset = get_windows(self.prepro_dataset)\n",
    "        self.data            = get_tensors_from_windows(self.windows_dataset)\n",
    "        self.transform       = transform\n",
    "        self.classes         = self.windows_dataset.datasets[0].windows.event_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data[0].shape[0]\n",
    "\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        # sample = {'signal': torch.from_numpy(self.data[0])[idx], 'label': torch.from_numpy(self.data[1])[idx]}\n",
    "        \n",
    "        sample = (torch.from_numpy(np.expand_dims(self.data[0], axis = 1))[idx], torch.from_numpy(self.data[1])[idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "102c03d8-2d0a-447d-9924-9b148dae2c26",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/braindecode/preprocessing/preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    }
   ],
   "source": [
    "my_eeg_data = EEG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2c75e9-6bbb-4112-91c6-38e80317c0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 400])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_eeg_data[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6779f-cf12-46eb-8f5d-7df3419484b6",
   "metadata": {},
   "source": [
    "# Creating CGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949ab9c-6290-43c3-b673-123d8e1cc7ae",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f43155b5-54c5-4f88-a13a-a7710e3f24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        im_chan: the number of channels of the output image, a scalar\n",
    "              (MNIST is black-and-white, so 1 channel is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, input_dim=68, im_chan=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(input_dim, hidden_dim * 4,      kernel_size = (1,60), stride = (1,1)),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size = (1,60), stride = (1,1)),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim,     kernel_size = (1,60), stride = (1,1)),\n",
    "            self.make_gen_block(hidden_dim, im_chan,            kernel_size = (3,50), stride = (1,2), padding = (0,2), final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size, stride, padding = 0, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, input_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "def get_noise(n_samples, input_dim, device='cpu'):\n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        device: the device type\n",
    "    '''\n",
    "    return torch.randn(n_samples, input_dim, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8fe5ac-5528-459c-949d-13847f7d5ae1",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "1db3e50e-82fe-45ee-913c-4388bead29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "      im_chan: the number of channels of the output image, a scalar\n",
    "            (MNIST is black-and-white, so 1 channel is your default)\n",
    "      hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, im_chan=5, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self.make_disc_block(im_chan, hidden_dim,        kernel_size = (1,50), stride = (2,4)),\n",
    "            self.make_disc_block(hidden_dim, hidden_dim * 2, kernel_size = (1,50), stride = (2,4)),\n",
    "            self.make_disc_block(hidden_dim * 2, 1,          kernel_size = (1,10), stride = (2,1), final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_disc_block(self, input_channels, output_channels, kernel_size, stride, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; \n",
    "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the discriminator: Given an image tensor, \n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with dimension (im_chan)\n",
    "        '''\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600c7bf-18b2-4491-ab0b-2aad906fb1b2",
   "metadata": {},
   "source": [
    "## Class Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a965b463-2fa0-4e62-bfc1-122b7daee376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_labels(labels, n_classes):\n",
    "    '''\n",
    "    Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes).\n",
    "    Parameters:\n",
    "        labels: tensor of labels from the dataloader, size (?)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "    '''\n",
    "    return F.one_hot(labels,n_classes)\n",
    "\n",
    "def combine_vectors(x, y):\n",
    "    '''\n",
    "    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?).\n",
    "    Parameters:\n",
    "      x: (n_samples, ?) the first vector. \n",
    "        In this assignment, this will be the noise vector of shape (n_samples, z_dim), \n",
    "        but you shouldn't need to know the second dimension's size.\n",
    "      y: (n_samples, ?) the second vector.\n",
    "        Once again, in this assignment this will be the one-hot class vector \n",
    "        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.\n",
    "    '''\n",
    "    combined = torch.cat((x.float(),y.float()), 1)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1193df6-7694-46be-890c-29f7c721f958",
   "metadata": {},
   "source": [
    "## Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c9d8dde-a8c9-4213-b908-049452ab3289",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_shape = (1, 3, 400)\n",
    "n_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "cd92c6ff-782e-479d-adbd-cd9839430257",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = \n",
    "z_dim = 64\n",
    "display_step = 100\n",
    "batch_size = 4\n",
    "lr = 0.0002\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b8efaf0-d4f6-4663-8afd-c0558f91e6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 1, 3, 400]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "dataloader = DataLoader(my_eeg_data, batch_size=batch_size,\n",
    "                        shuffle=True)\n",
    "\n",
    "for i_batch, (signal,label) in enumerate(dataloader):\n",
    "    print(i_batch, signal.size(),\n",
    "      label.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21f522d0-4f13-45a0-8952-a5cc62e29e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dimensions(z_dim, eeg_shape, n_classes):\n",
    "    '''\n",
    "    Function for getting the size of the conditional input dimensions \n",
    "    from z_dim, the image shape, and number of classes.\n",
    "    Parameters:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        eeg_shape: the shape of each EEG data as (C, W, H), which is (1, 3, 400), that is, we choose 3 channels (3 electrodes)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "                (4 for EEG, that is 4 movements - tongue, left hand, right hand and feet)\n",
    "    Returns: \n",
    "        generator_input_dim: the input dimensionality of the conditional generator, \n",
    "                          which takes the noise and class vectors\n",
    "        discriminator_im_chan: the number of input channels to the discriminator\n",
    "                            (e.g. C x 3 x 400 for EEG)\n",
    "    '''\n",
    "    generator_input_dim = z_dim + n_classes\n",
    "    discriminator_im_chan = eeg_shape[0] + n_classes\n",
    "    return generator_input_dim, discriminator_im_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "71d82a1b-122b-4d31-9af8-3991fc13944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, eeg_shape, n_classes)\n",
    "\n",
    "gen = Generator(input_dim=generator_input_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "disc = Discriminator(im_chan=discriminator_im_chan).to(device)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "fe5b3483-75e5-48cf-b938-a745eebf233d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 11/288 [00:00<00:05, 54.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 112/288 [00:01<00:03, 57.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: Generator loss: 1.1086874470114707, discriminator loss: 0.6132611681520939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 208/288 [00:03<00:01, 59.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: Generator loss: 1.5342509323358535, discriminator loss: 0.37836677342653274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:04<00:00, 58.59it/s]\n",
      "  7%|▋         | 20/288 [00:00<00:04, 59.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300: Generator loss: 1.485263275206089, discriminator loss: 0.43202665507793425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 122/288 [00:02<00:02, 59.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400: Generator loss: 0.9802088820934296, discriminator loss: 0.5686221241950988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 225/288 [00:03<00:01, 59.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: Generator loss: 1.9640622586011887, discriminator loss: 0.2801769895106554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:04<00:00, 59.79it/s]\n",
      " 12%|█▏        | 35/288 [00:00<00:04, 59.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600: Generator loss: 1.1931198585033416, discriminator loss: 0.43769308388233186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 136/288 [00:02<00:02, 59.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 700: Generator loss: 1.3431701636314393, discriminator loss: 0.4241561928391457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 231/288 [00:03<00:00, 59.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 800: Generator loss: 1.4111867016553878, discriminator loss: 0.33136028960347175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:04<00:00, 60.18it/s]\n",
      " 15%|█▍        | 42/288 [00:00<00:04, 60.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 900: Generator loss: 1.4303403747081758, discriminator loss: 0.37661984995007514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 147/288 [00:02<00:02, 60.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: Generator loss: 1.5958122992515564, discriminator loss: 0.2852784986048937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 243/288 [00:04<00:00, 60.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1100: Generator loss: 1.3973533642292022, discriminator loss: 0.3120411956310272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:04<00:00, 60.51it/s]\n",
      " 19%|█▉        | 55/288 [00:00<00:03, 60.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1200: Generator loss: 1.6292963576316835, discriminator loss: 0.28433141611516477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 158/288 [00:02<00:02, 61.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1300: Generator loss: 1.6418581819534301, discriminator loss: 0.23942606016993523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 256/288 [00:04<00:00, 60.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1400: Generator loss: 1.6842053627967835, discriminator loss: 0.22223169349133967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:04<00:00, 60.60it/s]\n"
     ]
    }
   ],
   "source": [
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for real, labels in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.to(device)\n",
    "    \n",
    "        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)\n",
    "        eeg_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        eeg_one_hot_labels = eeg_one_hot_labels.repeat(1, 1, eeg_shape[1], eeg_shape[2])\n",
    "    \n",
    "        ### Update discriminator ###\n",
    "        # Zero out the discriminator gradients\n",
    "        disc_opt.zero_grad()\n",
    "        # Get noise corresponding to the current batch_size \n",
    "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "    \n",
    "        # Now we can get the eeg data from the generator\n",
    "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
    "        #        2) Generate the conditioned fake eeg\n",
    "        noise_and_labels = combine_vectors(fake_noise, one_hot_labels)\n",
    "        fake = gen(noise_and_labels)\n",
    "    \n",
    "        # Now we can get the predictions from the discriminator\n",
    "        # Steps: 1) Create the input for the discriminator\n",
    "        #           a) Combine the fake images with image_one_hot_labels, \n",
    "        #              remember to detach the generator (.detach()) so you do not backpropagate through it\n",
    "        #           b) Combine the real images with image_one_hot_labels\n",
    "        #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n",
    "        #        3) Get the discriminator's prediction on the reals as disc_real_pred\n",
    "        \n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
    "        real_image_and_labels = combine_vectors(real, image_one_hot_labels)\n",
    "        disc_fake_pred = disc(fake_image_and_labels.detach())\n",
    "        disc_real_pred = disc(real_image_and_labels)\n",
    "    \n",
    "    \n",
    "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        disc_opt.step() \n",
    "    \n",
    "        # Keep track of the average discriminator loss\n",
    "        discriminator_losses += [disc_loss.item()]\n",
    "    \n",
    "        ### Update generator ###\n",
    "        # Zero out the generator gradients\n",
    "        gen_opt.zero_grad()\n",
    "    \n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
    "        # This will error if you didn't concatenate your labels to your image correctly\n",
    "        disc_fake_pred = disc(fake_image_and_labels)\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "    \n",
    "        # Keep track of the generator losses\n",
    "        generator_losses += [gen_loss.item()]\n",
    "    \n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
    "            print(f\"Step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\")\n",
    "        elif cur_step == 0:\n",
    "            print(\"Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\")\n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b55e2b-76e5-44ea-b700-f64118668c63",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "75c0ecd7-b5fc-405e-b468-35426d92e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before you explore, you should put the generator\n",
    "# in eval mode, both in general and so that batch norm\n",
    "# doesn't cause you issues and is using its eval statistics\n",
    "gen = gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2871b09-d01d-47ea-b8d1-8d7bc7337803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c7cd2bf-ea5e-4788-8567-9ff2be675e23",
   "metadata": {},
   "source": [
    "# Computing Dimensions for GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0034bb77-463b-4966-8a13-21467a558064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "def compute_dim_TransConv2D(Hin, \n",
    "                        kernel_size=1, \n",
    "                        stride=1,\n",
    "                        padding = 0,\n",
    "                        dilation = 1,\n",
    "                        output_padding = 0\n",
    "        \n",
    "        ):\n",
    "    return ((Hin - 1)*stride) - (2*padding)+ (dilation*(kernel_size - 1)) + output_padding + 1\n",
    "\n",
    "def compute_dim_Conv2D(Hin, \n",
    "                kernel_size=50, \n",
    "                stride=4,\n",
    "                padding = 0,\n",
    "                dilation = 1\n",
    "                ):\n",
    "    return floor( (Hin + (2*padding) - (dilation*(kernel_size-1)) -1)/(stride) + 1 ) \n",
    "\n",
    "## For EEG CG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2ca7b94f-a2cd-44d8-8f48-9b6728d84bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dim_Conv2D(compute_dim_Conv2D(compute_dim_Conv2D(400)), kernel_size = 10, stride = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "01009e9d-b342-4753-ab22-38727fefbf66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dim_TransConv2D(compute_dim_TransConv2D(compute_dim_TransConv2D(1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
