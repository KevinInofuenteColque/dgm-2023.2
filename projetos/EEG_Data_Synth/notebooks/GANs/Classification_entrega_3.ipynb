{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f8d000-1d1a-46b1-a3d0-e87f73ef5397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-21 04:01:36.209900: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-21 04:01:36.209927: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-21 04:01:36.211301: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-21 04:01:36.291285: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow not install, you could not use those pipelines\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale as standard_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.dataset import ValidSplit\n",
    "\n",
    "# torch.manual_seed(0) # Set for our testing purposes, please do not change!\n",
    "\n",
    "from braindecode.datasets import MOABBDataset\n",
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize, preprocess, Preprocessor)\n",
    "from braindecode.preprocessing import \\\n",
    "    create_windows_from_events, create_fixed_length_windows\n",
    "from braindecode.models import EEGNetv4\n",
    "from braindecode import EEGClassifier\n",
    "import mne\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52145935-7225-4c08-a9a9-d8eb0c738bb0",
   "metadata": {},
   "source": [
    "# Functions and Classes for EEG DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b03c552e-ccce-41ca-9f90-1c9263c17648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(\n",
    "    dataset,\n",
    "    low_cut_hz = 4.,   # low cut frequency for filtering\n",
    "    high_cut_hz = 38., # high cut frequency for filtering\n",
    "    newfreq = 100, # Paramater for resampling\n",
    "    factor = 1e6, # Parameter for scaling\n",
    "    ):\n",
    "\n",
    "    preprocessors = [\n",
    "        Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "        # Preprocessor(lambda data: np.multiply(data, factor)),  # Convert from V to uV\n",
    "        Preprocessor(\"resample\", sfreq=newfreq), # Resampling\n",
    "        Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
    "        Preprocessor(\"set_eeg_reference\", ref_channels=\"average\", ch_type=\"eeg\"), # Common Average Reference\n",
    "        Preprocessor(standard_scale, channel_wise=True) ## Standard Scale \n",
    "    ]\n",
    "\n",
    "    # Transform the data\n",
    "    # return preprocess(dataset, preprocessors, n_jobs = -1)\n",
    "    return preprocess(dataset, preprocessors)\n",
    "\n",
    "def get_windows(\n",
    "        dataset, \n",
    "        trial_start_offset_samples=0,\n",
    "        trial_stop_offset_samples=100,\n",
    "        window_size_samples=400,\n",
    "        window_stride_samples=100,\n",
    "        preload=True,\n",
    "        # mapping = {'left_hand': 0, 'right_hand': 1},\n",
    "        picks = ['C3', 'Cz', 'C4']\n",
    "        ):\n",
    "    \n",
    "    windows_dataset = create_windows_from_events(\n",
    "        dataset,\n",
    "        trial_start_offset_samples = trial_start_offset_samples,\n",
    "        trial_stop_offset_samples  = trial_stop_offset_samples,\n",
    "        window_size_samples        = window_size_samples,\n",
    "        window_stride_samples      = window_stride_samples,\n",
    "        preload                    = True,\n",
    "        # mapping = {'left_hand': 0, 'right_hand': 1},\n",
    "        # picks                      = picks\n",
    "        )\n",
    "\n",
    "    # preprocess(windows_dataset, [Preprocessor(standard_scale, channel_wise=True)]) ## Standard Scale window\n",
    "    \n",
    "    return windows_dataset\n",
    "\n",
    "\n",
    "def get_tensors_from_windows(windows_dataset):\n",
    "    windows_list = []\n",
    "    labels_list = []\n",
    "    n_runs = len(windows_dataset.datasets)\n",
    "    for i in range(n_runs):\n",
    "        windows_list.append(windows_dataset.datasets[i].windows.get_data())\n",
    "        labels_list.append(windows_dataset.datasets[i].y)\n",
    "        \n",
    "    stacked_tensor = np.concatenate(windows_list, axis=0)\n",
    "    stacked_labels = np.concatenate(labels_list, axis=0)\n",
    "    \n",
    "    del windows_list,labels_list\n",
    "    \n",
    "    return stacked_tensor, stacked_labels\n",
    "\n",
    "\n",
    "class EEG(Dataset):\n",
    "\n",
    "    def __init__(self, subject_id = 3, dataset_name=\"BNCI2014_001\", transform = None):\n",
    "        \n",
    "        self.raw_dataset     = MOABBDataset(dataset_name = dataset_name, subject_ids=subject_id)\n",
    "        self.prepro_dataset  = preprocessor(self.raw_dataset)\n",
    "        self.windows_dataset = get_windows(self.prepro_dataset)\n",
    "        self.data            = get_tensors_from_windows(self.windows_dataset)\n",
    "        self.transform       = transform\n",
    "        self.classes         = self.windows_dataset.datasets[0].windows.event_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data[0].shape[0]\n",
    "\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        # sample = {'signal': torch.from_numpy(self.data[0])[idx], 'label': torch.from_numpy(self.data[1])[idx]}\n",
    "        \n",
    "        sample = (torch.from_numpy(np.expand_dims(self.data[0], axis = 1))[idx], torch.from_numpy(self.data[1])[idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        im_chan: the number of channels of the output eeg, a scalar\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, input_dim=68, im_chan=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            #### For 3 channels\n",
    "            # self.make_gen_block(input_dim, hidden_dim * 4,      kernel_size = (1,60), stride = (1,1)),\n",
    "            # self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size = (1,60), stride = (1,1)),\n",
    "            # self.make_gen_block(hidden_dim * 2, hidden_dim,     kernel_size = (1,60), stride = (1,1)),\n",
    "            # self.make_gen_block(hidden_dim, im_chan,            kernel_size = (3,50), stride = (1,2), padding = (0,2), final_layer=True),\n",
    "            #### For 22 channels\n",
    "            self.make_gen_block(input_dim, hidden_dim * 4,      kernel_size = (3,60), stride = (1,1)),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size = (4,60), stride = (3,1)),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim,     kernel_size = (3,60), stride = (2,1)),\n",
    "            self.make_gen_block(hidden_dim, im_chan,            kernel_size = (2,50), stride = (1,2), padding = (0,2), final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size, stride, padding = 0, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, input_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0dfc26-a825-49cc-8640-47d53644e2f7",
   "metadata": {},
   "source": [
    "# Classification of the Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe41cc7d-c6b8-4066-8750-db3720f9d064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 165 samples (1.650 s)\n",
      "\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
     ]
    }
   ],
   "source": [
    "## Getting the real data\n",
    "my_eeg_data = EEG(subject_id=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87730745-dadf-4ca6-8207-941c5254a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = my_eeg_data[:][0].squeeze()\n",
    "y = my_eeg_data[:][1]\n",
    "\n",
    "real_complese_dataset =  TensorDataset(X,y)\n",
    "real_complete_train, real_test = random_split(real_complese_dataset, [0.5,0.5])\n",
    "real_train, real_eval = random_split(real_complete_train, [0.7,0.3])\n",
    "\n",
    "def get_train_test_eval(X,y):\n",
    "    \n",
    "    complese_dataset     =  TensorDataset(X,y)\n",
    "    complete_train, test = random_split(real_complese_dataset, [0.5,0.5])\n",
    "    train, eval          = random_split(real_complete_train, [0.7,0.3])\n",
    "\n",
    "    return train, test, eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe5910-c900-4124-b60d-8285dc6e34fd",
   "metadata": {},
   "source": [
    "# Creating Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2284b7e-505b-46da-88a6-f855a178ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "\n",
    "def train_model(train_dataset, eval_dataset, test_dataset):\n",
    "\n",
    "    # Classifier Config\n",
    "    n_epochs = 10\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Parameters configuration - Do not Change!\n",
    "    # Creating EEGNet model\n",
    "    model = EEGNetv4(\n",
    "        in_chans = 22,\n",
    "        n_classes = 4,\n",
    "        input_window_samples= 400,\n",
    "        final_conv_length='auto',\n",
    "        F1=8,\n",
    "        D=2,\n",
    "        F2=8,\n",
    "        kernel_length=64,\n",
    "        drop_prob=0.5\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # Creating classifier\n",
    "    EEGNetv4_classifier = EEGClassifier(\n",
    "        model,\n",
    "        criterion=torch.nn.NLLLoss,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        train_split = predefined_split(eval_dataset),\n",
    "        # train_split = ValidSplit(0.2),\n",
    "        batch_size = batch_size,\n",
    "        callbacks=[\n",
    "            \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "        ],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Trainning\n",
    "    EEGNetv4_classifier.fit(train_dataset, y=None, epochs=n_epochs)\n",
    "\n",
    "    # Computing accuracy\n",
    "    acc = np.mean(EEGNetv4_classifier.predict(test_dataset) == [label for X,label in test_dataset])\n",
    "    \n",
    "\n",
    "    return EEGNetv4_classifier,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4346580-687e-4161-ac9a-0f9ae41d442c",
   "metadata": {},
   "source": [
    "# Loading Weights for Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9cd355c-6494-4eaf-8be3-290a88c29df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generator Class\n",
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        im_chan: the number of channels of the output eeg, a scalar\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, input_dim=68, im_chan=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            #### For 3 channels\n",
    "#             self.make_gen_block(input_dim, hidden_dim * 4,      kernel_size = (1,60), stride = (1,1)),\n",
    "#             self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size = (1,60), stride = (1,1)),\n",
    "#             self.make_gen_block(hidden_dim * 2, hidden_dim,     kernel_size = (1,60), stride = (1,1)),\n",
    "#             self.make_gen_block(hidden_dim, im_chan,            kernel_size = (3,50), stride = (1,2), padding = (0,2), final_layer=True),\n",
    "            #### For 22 channels\n",
    "            self.make_gen_block(input_dim, hidden_dim * 4,      kernel_size = (3,60), stride = (1,1)),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size = (4,60), stride = (3,1)),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim,     kernel_size = (3,60), stride = (2,1)),\n",
    "            self.make_gen_block(hidden_dim, im_chan,            kernel_size = (2,50), stride = (1,2), padding = (0,2), final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size, stride, padding = 0, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, input_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "def get_noise(n_samples, input_dim, device='cpu'):\n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        device: the device type\n",
    "    '''\n",
    "    return torch.randn(n_samples, input_dim, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c6b4d7-9132-41c4-9de7-659ee56a2741",
   "metadata": {},
   "source": [
    "### Some other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b1605b-f2d5-4ed4-a6a5-6ca752c75813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_labels(labels, n_classes):\n",
    "    '''\n",
    "    Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes).\n",
    "    Parameters:\n",
    "        labels: tensor of labels from the dataloader, size (?)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "    '''\n",
    "    return F.one_hot(labels,n_classes)\n",
    "\n",
    "def combine_vectors(x, y):\n",
    "    '''\n",
    "    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?).\n",
    "    Parameters:\n",
    "      x: (n_samples, ?) the first vector. \n",
    "        In this assignment, this will be the noise vector of shape (n_samples, z_dim), \n",
    "        but you shouldn't need to know the second dimension's size.\n",
    "      y: (n_samples, ?) the second vector.\n",
    "        Once again, in this assignment this will be the one-hot class vector \n",
    "        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.\n",
    "    '''\n",
    "    combined = torch.cat((x.float(),y.float()), 1)\n",
    "    return combined\n",
    "\n",
    "\n",
    "# def generate_samples_with_labels(label, n_samples, generator, z_dim = 64, channel = None, extra_dim = True):\n",
    "#     '''\n",
    "#     Function for generating samples, once the generator has been trained\n",
    "#         label: label of the movement to be sampled. See dictionary below\n",
    "#         {'feet': 0, 'left_hand': 1, 'right_hand': 2, 'tongue': 3}\n",
    "#         n_samples: number of samples to be generated\n",
    "#         channel: electrode {'C3': 0, 'Cz': 1, 'C4': 2} -> Default: All channels\n",
    "#         generator: the trained generator\n",
    "#     '''\n",
    "\n",
    "#     n_classes = 4\n",
    "\n",
    "#     if channel == None:\n",
    "#         noise_4_gen = get_noise(n_samples, z_dim)\n",
    "#         label = get_one_hot_labels(torch.Tensor([label]).long(), n_classes).repeat(n_samples,1)\n",
    "\n",
    "#         noise_and_labels = combine_vectors(noise_4_gen, label)\n",
    "#         fake = generator(noise_and_labels)\n",
    "\n",
    "#         if extra_dim == False:\n",
    "#             fake = fake.reshape((fake.shape[0], fake.shape[2], fake.shape[3]))\n",
    "#         return fake\n",
    "#     else:\n",
    "#         noise_4_gen = get_noise(n_samples, z_dim)\n",
    "#         label = get_one_hot_labels(torch.Tensor([label]).long(), n_classes).repeat(n_samples,1)\n",
    "\n",
    "#         noise_and_labels = combine_vectors(noise_4_gen, label)\n",
    "#         fake = generator(noise_and_labels)\n",
    "#         filtered_channel_fake = torch.select(fake, dim = 2, index = channel)\n",
    "\n",
    "#         if extra_dim == False:\n",
    "#             filtered_channel_fake = filtered_channel_fake.reshape((filtered_channel_fake.shape[0], filtered_channel_fake.shape[2]))\n",
    "\n",
    "#         return filtered_channel_fake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d1427-787b-46ad-9319-6632e66f3c79",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18823394-b0ad-427f-9f70-2c1135df715e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"../Weights/Conv_Cond_Gan_v4_0.5loss/gen_22ch_batch64_lr37e6_decay7.pt\"\n",
    "gen = Generator()\n",
    "gen.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a3d7f-ab23-4743-bd25-e1d0e188b40e",
   "metadata": {},
   "source": [
    "## Function for Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cada814-fc9c-4842-a0cc-e9c72fc7595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_for_classification(n_samples, generator):\n",
    "    '''\n",
    "    Function for generating equal label samples for the classifier.\n",
    "        n_samples: number of samples to be generated\n",
    "        generator: the trained generator\n",
    "    '''\n",
    "    n_classes = 4\n",
    "    z_dim = 64\n",
    "    \n",
    "    n_samples_partial = int(n_samples/n_classes)\n",
    "    noise_4_gen = get_noise(n_samples, z_dim)\n",
    "\n",
    "    label = [0,1,2,3]\n",
    "\n",
    "    label = [get_one_hot_labels(torch.Tensor([i]).long(), n_classes).repeat(n_samples_partial,1) for i in label]\n",
    "\n",
    "    label_concat = torch.zeros_like(torch.Tensor(0,4))\n",
    "    for i in range(len(label)):\n",
    "        label_concat = torch.cat((label_concat,label[i]), 0)\n",
    "\n",
    "    noise_and_labels = combine_vectors(noise_4_gen, label_concat)\n",
    "\n",
    "    fake = generator(noise_and_labels)\n",
    "\n",
    "    labels = torch.argmax(label_concat,dim = 1)\n",
    "    return (fake, labels)\n",
    "\n",
    "\n",
    "def data_augment(real_dataset, real_labels, augment_percentage, generator):\n",
    "    '''\n",
    "    Function for adding fake data into the real data, with percentage \"augment_percentage\".\n",
    "    real_dataset: dataset of real data\n",
    "    real_labels: labels of real data\n",
    "    augment_percentage: percentage of real data to be added as fake data.\n",
    "    generator: the trained generator\n",
    "    '''\n",
    "    # parameters\n",
    "    n_classes = 4\n",
    "\n",
    "    # Get real dataset size\n",
    "    real_dataset_size = real_dataset.shape[0]\n",
    "    # Compute size of augmentation\n",
    "    augment_size = augment_percentage*real_dataset_size\n",
    "    # If the size is divisible by the number of classes (4) continue, else choose the closest divisible number\n",
    "    if augment_size % n_classes != 0:\n",
    "        augment_size = int(augment_size/4.0)\n",
    "    print(\"Size of augmentation:\",augment_size)\n",
    "\n",
    "    # Generate fake data\n",
    "    fake_data, fake_labels = generate_samples_for_classification(augment_size, generator)\n",
    "\n",
    "    fake_data = fake_data.squeeze()\n",
    "    \n",
    "    # Concatenate real and fake data\n",
    "    augmented_dataset = torch.cat((real_dataset,fake_data), dim = 0)\n",
    "    augmented_labels  = torch.cat((real_labels,fake_labels), dim = 0)\n",
    "    print(\"Final augmented dataset shape:\",augmented_dataset.shape)\n",
    "    \n",
    "    return augmented_dataset,augmented_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80302c02-0df9-4c06-8023-bfe5dc096224",
   "metadata": {},
   "source": [
    "### Generate variable augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "897b75e5-6363-42b8-ac0e-3fa067efd090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augment 0 %\n",
      "Size of augmentation: 0\n",
      "Final augmented dataset shape: torch.Size([1152, 22, 400])\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3416\u001b[0m        \u001b[32m1.4371\u001b[0m            \u001b[35m0.3372\u001b[0m        \u001b[31m1.3776\u001b[0m  0.0100  0.4916\n",
      "      2            \u001b[36m0.5099\u001b[0m        \u001b[32m1.3265\u001b[0m            \u001b[35m0.4942\u001b[0m        \u001b[31m1.2994\u001b[0m  0.0097  0.4945\n",
      "      3            \u001b[36m0.5322\u001b[0m        \u001b[32m1.1907\u001b[0m            \u001b[35m0.5349\u001b[0m        \u001b[31m1.1655\u001b[0m  0.0088  0.4878\n",
      "      4            0.4950        \u001b[32m1.0818\u001b[0m            0.5000        \u001b[31m1.0767\u001b[0m  0.0075  0.5091\n",
      "      5            \u001b[36m0.5371\u001b[0m        \u001b[32m1.0626\u001b[0m            \u001b[35m0.5407\u001b[0m        \u001b[31m1.0325\u001b[0m  0.0059  0.4968\n",
      "      6            \u001b[36m0.6114\u001b[0m        \u001b[32m0.9801\u001b[0m            \u001b[35m0.6047\u001b[0m        \u001b[31m0.9938\u001b[0m  0.0041  0.4881\n",
      "      7            \u001b[36m0.6733\u001b[0m        \u001b[32m0.9759\u001b[0m            \u001b[35m0.6105\u001b[0m        \u001b[31m0.9868\u001b[0m  0.0025  0.4963\n",
      "      8            \u001b[36m0.6906\u001b[0m        \u001b[32m0.9533\u001b[0m            \u001b[35m0.6163\u001b[0m        \u001b[31m0.9820\u001b[0m  0.0012  0.4887\n",
      "      9            \u001b[36m0.6980\u001b[0m        \u001b[32m0.9182\u001b[0m            \u001b[35m0.6221\u001b[0m        0.9850  0.0003  0.4987\n",
      "     10            0.6980        0.9593            \u001b[35m0.6279\u001b[0m        0.9900  0.0000  0.4881\n",
      "Augment 0.07 %\n",
      "Size of augmentation: 20\n",
      "Final augmented dataset shape: torch.Size([1172, 22, 400])\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2574\u001b[0m        \u001b[32m1.3962\u001b[0m            \u001b[35m0.2093\u001b[0m        \u001b[31m1.3920\u001b[0m  0.0100  0.4896\n",
      "      2            \u001b[36m0.5000\u001b[0m        \u001b[32m1.3360\u001b[0m            \u001b[35m0.4012\u001b[0m        \u001b[31m1.3228\u001b[0m  0.0097  0.4913\n",
      "      3            0.3738        \u001b[32m1.2202\u001b[0m            0.3895        \u001b[31m1.2712\u001b[0m  0.0088  0.4981\n",
      "      4            0.3663        \u001b[32m1.1444\u001b[0m            0.3721        1.2900  0.0075  0.4918\n",
      "      5            0.4455        \u001b[32m1.0927\u001b[0m            \u001b[35m0.4244\u001b[0m        \u001b[31m1.2224\u001b[0m  0.0059  0.5079\n",
      "      6            \u001b[36m0.5421\u001b[0m        \u001b[32m1.0445\u001b[0m            \u001b[35m0.4593\u001b[0m        \u001b[31m1.1302\u001b[0m  0.0041  0.4916\n",
      "      7            \u001b[36m0.5718\u001b[0m        \u001b[32m1.0136\u001b[0m            \u001b[35m0.4709\u001b[0m        \u001b[31m1.0916\u001b[0m  0.0025  0.4949\n",
      "      8            \u001b[36m0.6139\u001b[0m        1.0341            \u001b[35m0.5407\u001b[0m        \u001b[31m1.0496\u001b[0m  0.0012  0.5048\n",
      "      9            \u001b[36m0.6386\u001b[0m        \u001b[32m0.9900\u001b[0m            \u001b[35m0.5465\u001b[0m        \u001b[31m1.0286\u001b[0m  0.0003  0.5048\n",
      "     10            \u001b[36m0.6559\u001b[0m        1.0214            \u001b[35m0.5640\u001b[0m        \u001b[31m1.0197\u001b[0m  0.0000  0.4970\n",
      "Augment 0.1 %\n",
      "Size of augmentation: 28\n",
      "Final augmented dataset shape: torch.Size([1180, 22, 400])\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3045\u001b[0m        \u001b[32m1.4428\u001b[0m            \u001b[35m0.3081\u001b[0m        \u001b[31m1.3766\u001b[0m  0.0100  0.4896\n",
      "      2            \u001b[36m0.3713\u001b[0m        \u001b[32m1.3418\u001b[0m            \u001b[35m0.3140\u001b[0m        \u001b[31m1.3225\u001b[0m  0.0097  0.4965\n",
      "      3            0.3020        \u001b[32m1.2556\u001b[0m            0.2849        1.3249  0.0088  0.4924\n",
      "      4            \u001b[36m0.4480\u001b[0m        \u001b[32m1.2304\u001b[0m            \u001b[35m0.4302\u001b[0m        \u001b[31m1.1732\u001b[0m  0.0075  0.4962\n",
      "      5            \u001b[36m0.5099\u001b[0m        \u001b[32m1.1308\u001b[0m            \u001b[35m0.4477\u001b[0m        \u001b[31m1.1405\u001b[0m  0.0059  0.4995\n",
      "      6            \u001b[36m0.6139\u001b[0m        \u001b[32m1.1263\u001b[0m            \u001b[35m0.5349\u001b[0m        \u001b[31m1.0699\u001b[0m  0.0041  0.4938\n",
      "      7            \u001b[36m0.6238\u001b[0m        \u001b[32m1.0598\u001b[0m            \u001b[35m0.5523\u001b[0m        \u001b[31m1.0343\u001b[0m  0.0025  0.5023\n",
      "      8            \u001b[36m0.6485\u001b[0m        \u001b[32m1.0175\u001b[0m            \u001b[35m0.5988\u001b[0m        \u001b[31m1.0174\u001b[0m  0.0012  0.5067\n",
      "      9            \u001b[36m0.6658\u001b[0m        1.0441            \u001b[35m0.6105\u001b[0m        \u001b[31m1.0133\u001b[0m  0.0003  0.5078\n",
      "     10            0.6634        1.0903            \u001b[35m0.6163\u001b[0m        \u001b[31m1.0124\u001b[0m  0.0000  0.5073\n"
     ]
    }
   ],
   "source": [
    "classification_df = pd.DataFrame(columns = [\"Augmentation (%)\", \"EEGNet Accuracy\"] )\n",
    "\n",
    "augment_percentages = [0,0.07,0.1,0.21,0.53,0.71,1,2]\n",
    "\n",
    "\n",
    "for count, i in enumerate(augment_percentages):\n",
    "    print(\"Augment\", i, \"%\")\n",
    "    dataset_augmented, labels_augmented = data_augment(X,y,i,gen)\n",
    "    train_aug, test_aug, eval_aug = get_train_test_eval(dataset_augmented, labels_augmented)\n",
    "    model, acc = train_model(train_aug, eval_aug, test_aug)\n",
    "    classification_df.loc[count] = [i,acc]\n",
    "\n",
    "\n",
    "classification_df.to_csv(\" classification_results.csv\", index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8921b173-9d31-466c-9a6c-54d9bbc51e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df.to_csv(\" classification_results.csv\", index = False )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
